%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass{bmcart}
%%% Load packages
%\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
% \usepackage[utf8]{inputenc} %unicode support
\usepackage[utf8x]{inputenc}
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails
\usepackage{bm}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\newcommand{\bb}{\bm{\beta}}
\usepackage{mathptmx}  
\usepackage{mathptmx}  
\usepackage{graphicx}
\usepackage{mathptmx}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amssymb,amsmath,amsfonts}
\usepackage{setspace}
\usepackage{amsthm}
\usepackage{pdfpages}
\usepackage{subfig}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem*{ap}{Alternative presentation}
\DeclareMathOperator*{\argmin}{arg\,min}
% for strikethrough
\usepackage[normalem]{ulem}
% for colored comments
\newenvironment{note}{\par\color{blue}}{\par}
\newenvironment{old}{\par\color{orange}}{\par}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\def\includegraphic{}
%\def\includegraphics{}

\newcommand{\A}{\mathbf{A}}
\newcommand{\G}{\mathbf{G}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\X}{\mathbf{H}_{22}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\h}{\mathbf{h}}
\renewcommand{\P}{\mathbf{P}}
\newcommand{\e}{\bm{\epsilon}}
%%% Put your definitions there:
\startlocaldefs
\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research Article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Lost in translation: On the impact of data coding on penalized regression methods with interactions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1,aff2},                   % id's of addresses, e.g. {aff1,aff2}
   noteref={n1},                        % id's of article notes, if any
   email={jmartin2@gwdg.de}   % email address
]{\inits{JE}\fnm{ Johannes WR } \snm{Martini}}
\author[
addressref={aff3},
noteref={n1},
]{Francisco Rosales}
\author[
addressref={aff1},
noteref={n1},
]{Ngoc-Thuy Ha}
\author[
   addressref={aff4},
]{Thomas Kneib}
\author[
addressref={aff5},
]{Johannes Heise}
\author[addressref={aff2}]{Valentin Wimmer}
\author[
addressref={aff1},
]{Henner Simianer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgname{University of Goettingen, Department of Animal Breeding and Genetics}, % university, etc                    %
  %\postcode{}                                % post or zip code                              % city
  \cny{Germany}                                    % country
}

\address[id=aff2]{%
	\orgname{KWS SAAT SE},
	\city{Einbeck},
	\cny{Germany}
}
\address[id=aff3]{%
  \orgname{Departamento de Producción Animal, Facultad de Agronomía, Universidad de Buenos Aires},
  \city{Buenos Aires},
  \cny{Argentina}
}

\address[id=aff4]{%
	\orgname{Bavarian State Research Center for Agriculture, Institute of Animal Breeding},
	\city{Poing‑Grub},
	\cny{Germany}
}

\address[id=aff5]{%
	\orgname{CONICET},
	\cny{Argentina}
}

\address[id=aff6]{%
	\orgname{IGEVET - Instituto de Genética Veterinaria (UNLP-CONICET LA PLATA), Facultad de Ciencias Veterinarias},
	\city{La Plata},
	\cny{Argentina}
}

\address[id=aff7]{%
	\orgname{INPA, UBA-CONICET},
	\cny{Argentina}
}


\address[id=aff8]{%
	\orgname{National Engineering Research Center for Breeding Swine Industry, Guangdong Provincial Key Lab of Agro-animal Genomics and Molecular Breeding, College of Animal Science, South China Agricultural University},
	\city{Guangzhou},
	\cny{China}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{artnotes}
%\note{Sample of title note}     % note to the article
\note[id=n1]{Equal contributor} % note, connected to author
\end{artnotes}

\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract

{\bf Background} Penalized regression approaches are standard tools in quantitative genetics.  
It is known that the fit of an \emph{ordinary least squares} (OLS) regression is independent of certain transformations of the coding of the predictor variables, and that the standard mixed model \emph{ridge regression best linear unbiased prediction} (RRBLUP) is neither affected by translations of the variable coding, nor by global scaling. 
However, it has been reported that an extended version of this mixed model, which incorporates interactions by products of markers as additional predictor variables, indeed is affected by translations of the marker coding. 


{\bf Results} In this work, we identify the cause of this loss of invariance in a general context of mixed models defined on polynomials in the predictor variables. We show that in most cases, translating the coding of the predictor variables has an impact on penalized regressions, with the exception of the situation in which only the size of the coefficients of monomials of highest degree are penalized. The invariance of RRBLUP 
can thus be considered as a special case of this setting, that is as a polynomial of degree 1, where the size of the fixed effect (degree 0) is not penalized but all coefficients of monomials of degree 1 are.
The extended RRBLUP which includes interactions is not invariant to translations, since it does not only penalize interactions (degree 2), but also additive effects (degree 1). Finally, we investigate the impact of changes of the coding on estimated effect sizes in a pair epistasis model on a publicly available wheat data set.

{\bf Conclusion}
 Our results give a general insight into the behavior of penalized regressions. The fact that coding translations alter the estimates of interaction effects, provides an additional reason to interpret the biological meaning of these interactions with caution. Moreover, this problem does not only apply to gene by gene interactions, but also to other types of interactions modeled in mixed models with Hadamard products of covariance matrices (for instance gene by environment interactions).
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{epistasis}
\kwd{extended GBLUP}
\kwd{coding-dependence}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
%\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%-
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
\newpage
\section*{Background}
\emph{Genomic prediction}, that is the prediction of properties of individuals from their genetic data, is a crucial ingredient of modern breeding programs \cite{meuwissen01,schaeffer06,habier07,hayes09a,hayes13}.
The traditional quantitative genetics theory is built upon linear models in which allele effects are usually modeled additively \cite{falconer96}.
In particular,
the usual model to represent the effect of the genotype on the phenotype is given by
\begin{equation} \label{eq:01}
	\mathbf{y}=\mathbf{1}_n \mu + \M \bm{\beta} + \bm{\epsilon},
\end{equation} 
where $\mathbf{y}$ is the $n \times 1$ vector of the phenotypic observations of $n$ individuals and $\mathbf{1}_n$ an $n \times 1$ vector with each entry equal to $1$.  Moreover,
$\mu$ is the $y$-intercept, and $\M$  the  $n \times p$ matrix describing the marker states of $n$ individuals at $p$ loci. Dealing with single nucleotide polymorphisms (SNPs) and a diploid species, the entries $M_{i,j}$ can for instance be coded as $0$ $(\mathbf{aa})$, $1$ $(\mathbf{aA}$ or $\mathbf{Aa})$ or $2$ $(\mathbf{AA})$ counting the occurrence of the reference allele $\mathbf{A}$. The $p \times 1$ vector $\bm{\beta}$ represents the allele substitution effects of the $p$ loci, and $\bm{\epsilon}$ the $n \times 1$ error vector. For single marker regression, which may for instance be used in \emph{genome wide association studies} (GWAS), we can apply ordinary least square regression to determine $\beta$. 
However, in approaches of genomic prediction, we model the effects of many different loci simultaneously and the number of markers $p$ is usually much larger than the number of observations $n$. To reduce overfitting and to deal with a large number of predictor variables, different methods have been applied in the last decades, among which \emph{ridge regression best linear unbiased prediction} (RRBLUP) is the most popular \cite{schaeffer2004application}. RRBLUP penalizes the squared $\ell_2$ norm of $\bm{\beta}$ and is built on the additional model specifications of $\mu$ being a fixed unknown parameter, $\bm{\beta}\sim \mathcal{N}(\bm{0},\sigma_\beta^2 \mathbf{I})$ and $\bm{\epsilon}\sim \mathcal{N}(\bm{0},\sigma_\epsilon^2 \mathbf{I})$ with $\mathbf{I}$ the identity matrix.
With an approach of maximizing a certain density, these assumptions allow to derive the optimal penalty factor as the ratio of the variance components $\lambda:= \frac{\sigma_\epsilon^2}{\sigma_\beta^2}$ \cite{henderson75,henderson76,henderson77}. In practice these variance components are usually estimated from the data, albeit the theory to derive the optimal penalty is based on the assumption of $\sigma_\beta^2$ and $\sigma_\epsilon^2$ being known. Note here that the fixed effect $\mu$ is not penalized in RRBLUP, which means that this method is not a pure ridge regression but actually a mixed model in which the size of $\mu$ is not penalized but the entries of $\bm{\beta}$ are.  
This mixed model RRBLUP is also called \emph{genomic best linear unbiased prediction} (GBLUP) when the model is reformulated with $\mathbf{g}:=\M \bm{\beta}$, and thus $\mathbf{g}\sim \mathcal{N}(0,\sigma_\beta^2 \M' \M)$. \\

It is known that translations of the marker coding, that is subtracting a constant $p_i$ from the $i$-th column of $\M$ does not change the predictions $\hat{\y}$ of an  OLS regression (provided it is well-defined). This invariance also holds for RRBLUP, when the variance components remain unchanged. However, when modeling interactions by products of two predictor variables, that is when fitting the coefficients of a polynomial of degree two to the data, OLS predictions are not affected by translations of the marker coding, but the predictions of its penalized regression analogue \emph{extended genomic best linear unbiased prediction} (EGBLUP) indeed are sensible to a translation of the coding \cite{he2016does,Martini17}.\\

In this work we will address the question of why the penalized regression method is affected by translations of the marker coding when a polynomial function of higher degree is used. 
We will start with a short recapitulation of the different methods.



\section*{Theory: Specification of regression methods}
In the following we specify the relevant models and regressions to answer the research question previously stated. 
If an expression includes an inverse of a matrix, we implicitly assume that the matrix is invertible for the respective statement, also if not mentioned explicitly. 
Analogously, some statements for OLS may implicitly assume that a unique estimate exists, which for instance implicitly restricts to cases of $n>p$ for OLS. \vspace{0.4cm} \\
{\bf Additive effect regression}\vspace{0.2cm}\\ 
The additive effect model has already been presented in Eq.~(\ref{cor:01}) \vspace{0.1cm}\\
{\bf OLS} 
The ordinary least squares approach is to determine $\bm{\beta}$ by minimizing the squared residuals: 
\begin{equation}\label{eq:03}
	\begin{pmatrix}
		\hat{\mu} \\
		\hat{\bm{\beta}}
	\end{pmatrix}_{\mbox{\small OLS}} := \argmin_{(\mu,\bm{\beta}) \in \mathbb{R}^{p+1}} \sum\limits_{i=1}^n (y_i - \M_{i,\bullet}\bm{\beta} - \mu)^2
\end{equation}
$\M_{i,\bullet}$ denotes here the $i$-th row of $\M$, that is the genomic data of individual $i$. The solution to the minimization problem of Eq.~(\ref{eq:03}) is given by the well-known OLS estimate
\begin{equation}\label{eq:04}
	\begin{pmatrix}
		\hat{\mu} \\
		\hat{\bm{\beta}}
	\end{pmatrix}_{\mbox{\small OLS}} =
	\left( 
	\begin{pmatrix}
		\mathbf{1}_n & \hspace{-0.2cm}\M
	\end{pmatrix}^{t}
	\begin{pmatrix}
		\mathbf{1}_n & \hspace{-0.2cm} \M
	\end{pmatrix} \right)^{-1} 
	\begin{pmatrix}
		\mathbf{1}_n & \hspace{-0.2cm} \M
	\end{pmatrix}^{t}
	\mathbf{y}
\end{equation}
provided that the required inverse exists, which in particular also means that $n$ has to be greater than $p$.\\

In problems of statistical genetics, we often deal with a high number of loci and a relatively low number of observations.
In this situation of $p + 1 > n$, the solution to Eq.~(\ref{eq:03}) is not unique but a vector subspace of which each point minimizes Eq.~(\ref{eq:03}) to zero.
Due to this overfit, the quality of predictions $\hat{\mathbf{y}}$ for genotypes which have not been used to estimate the parameter $(\hat{\mu},\hat{\beta})$ are usually poor. An approach to overcome this problem is RRBLUP.
\vspace{0.4cm}\\
{\bf RRBLUP / GBLUP} minimizes 
\begin{equation}\label{eq:05}
	\begin{pmatrix}
		\hat{\mu} \\
		\hat{\bm{\beta}}
	\end{pmatrix}_{\mbox{\small RR}_\lambda} :=\argmin_{(\mu,\bm{\beta}) \in \mathbb{R}^{p+1}} \sum\limits_{i=1}^n (y_i - \M\bm{\beta} - \mu)^2  + \lambda \sum\limits_{j=1}^p {\beta}_j^2
\end{equation}
for a penalty factor $\lambda > 0$. Using an approach of maximizing the density of the joint distribution of $(\mathbf{y},\bm{\beta})$, the model specifications of ${\beta_i}\stackrel{i.i.d.}{\sim} \mathcal{N}(0,\sigma_\beta^2)$ and ${\epsilon_i}\stackrel{i.i.d.}{\sim} \mathcal{N}(0,\sigma_\epsilon^2)$ allow to determine the penalty factor as ratio of the variance components  as $\lambda := \frac{\sigma_\epsilon^2}{\sigma_\beta^2}$. We stress that Eq.~(\ref{eq:05}) is not a pure ridge regression (RR), as the name RRBLUP might suggest, but a mixed model which treats $\mu$ and $\bb$ differently by not penalizing the size of $\mu$. This is the version which is most frequently used in the context of genomic prediction (often with additional fixed effects). \\

The corresponding solution is given by 
\begin{equation}
	\begin{pmatrix}
		\hat{\mu} \\
		\hat{\bm{\beta}}
	\end{pmatrix}_{\mbox{\small RR}_\lambda} =
	\left( 
	\begin{pmatrix}
		\mathbf{1}_n & \hspace{-0.2cm}\M
	\end{pmatrix}^{t}
	\begin{pmatrix}
		\mathbf{1}_n & \hspace{-0.2cm} \M
	\end{pmatrix} + \lambda \begin{pmatrix}
		0 & \mathbf{0 }_p^t \\
		\mathbf{0 }_p & \mathbf{I}_p
	\end{pmatrix} 
	\right)^{-1} 
	\begin{pmatrix}
		\mathbf{1}_n & \hspace{-0.2cm} \M
	\end{pmatrix}^{t}
	\mathbf{y}. \label{eq:06}
\end{equation}
Here, $\mathbf{0 }_p$ denotes the $p \times 1$ vector of zeros and $\mathbf{I}_p$ the $p$-dimensional identity matrix. 
The effect of the introduction of the penalization term $\lambda \sum\limits_{i=1}^p {\beta}_i^2$ is that for the minimization of Eq.~(\ref{eq:05}), we have a trade-off between fitting the data optimally and shrinking the square effects to $0$. The method will only ``decide'' to increase the estimate $\hat{\beta}_j$, if the gain from improving the fit is greater than the penalized loss generated by the increase of $\hat{\beta}_j$.  \vspace{0.4cm} \\

{\bf First order epistasis: Polynomials of degree two}\vspace{0.2cm}\\ 
An extension of the additive model of Eq.~(\ref{eq:01}) is a first order epistasis model given by a polynomial of degree two in the marker data \cite{ober15,jiang15,Martini16}
\begin{equation} \label{eq:07}
	y_i =\mathbf{1}_n \mu + \M_{i,\bullet}\bm{\beta} + \sum\limits_{k=1}^p\sum\limits_{j=k+1}^p h_{j,k}M_{i,j}M_{i,k} + \bm{\epsilon}
\end{equation}\vspace{0.1cm}\\
{\bf OLS} Since the model is still linear in the coefficients, Eq.~(\ref{eq:04}) represents the OLS solution, but with a modified matrix $\M$ including the products of markers as additional predictor variables. \vspace{0.4cm}\\
{\bf eRRBLUP}
The extended RRBLUP is based on the additional assumption of $h_{j,k }\stackrel{i.i.d.}{\sim} \mathcal{N}(0,\sigma_h^2 )$. Also here the solution is given by an analogon of Eq.~(\ref{eq:06}), but with two different penalty factors $\lambda_1:=\frac{\sigma_\epsilon^2}{\sigma_\beta^2}$ and $\lambda_2:=\frac{\sigma_\epsilon^2}{\sigma_h^2}$.
\vspace{0.4cm}\\

{\bf Translations of the marker coding}\vspace{0.2cm}\\
In quantitative genetics, often allele frequencies are subtracted from the original $0$, $1$, $2$ coding of $\M$ to 
use $\tilde{\M}:=\M-\mathbf{1}_n\P^t$ with $\P$ the vector of column means of $\M$ such that $$\sum\limits_{i=1,...,n} \tilde{M}_{i,j} = 0\; \forall j \in \{1,...,p\}.$$ However, also other types of translations, for instance a symmetric $\{-1,0,1\}$ coding or a genotype-frequency centered coding \cite{vitezica} can be found in quantitative genetics literature. Thus, the question occurs whether this has an impact on the estimates of the marker effects or on the prediction of new genotypes. \\

The answer is that for the additive setup of Eq.~(\ref{eq:01}), a shift from $\M$ to $\tilde{\M}$ will change $\hat{\mu}$ but not $\hat{\bm{\beta}}$ and any prediction $\hat{\mathbf{y}}$ will not be affected, neither for OLS, nor for RRBLUP (provided that $\lambda$ is not changed). This invariance we observe for the additive model does not hold for the extended RRBLUP method.

We will give an example and discuss the effect of translations of the marker coding in a more general way afterwards. \\

\begin{example}[Translations of the marker coding]\label{ex:06}
	Let the marker data of five individuals with two markers be given:
	$$\mathbf{y}= (-0.72,2.34,0.08,-0.89,0.86)^t \qquad 
	\M = \begin{pmatrix}
	2 & 2 \\
	1 & 2 \\
	2 & 0 \\
	2 & 1 \\
	1 & 0 \\
	\end{pmatrix} $$ 
	Moreover, let us use the original matrix $\M$, and the by allele frequencies centered matrix $\tilde{\M}:= \M - \mathbf{1}\underbrace{(1.6,1)}_{=:\P^t}$.
	We consider the first order epistasis model $$y_i := \mu + \beta_1 M_{i,1} + \beta_2 M_{i,2}  + h_{1,2} M_{i,1}M_{i,2} + \epsilon_i.$$
	Then, we obtain  the corresponding estimates based on i) an OLS model, ii) a the mixed model RRBLUP1 of Eq.~(\ref{eq:06}) with $\lambda=1$, and iii) a  mixed model RRBLUP2 of EQ.~(\ref{eq:06}) with $\lambda=1$ when $\beta_1$ and $\beta_2$ are non-penalized. The results are reported in table~\ref{table:example1}. The columns labeled ``centred" correspond to the usage of design matrix $\M$, and the columns labeled ``uncentered" correspond to the usage of design matrix $\tilde\M$.

\begin{table}[ht] \caption{Results from Example 1}\label{table:example1}
\centering
\begin{tabular}{|l|r|r|r||r|r|r|}
\hline
&\multicolumn{3}{c||}{Non-Centred ($\M$)}&\multicolumn{3}{c|}{Centred ($\tilde\M$)}\\ 
\hline
Coeffs.&OLS&RRBLUP1&RRBLUP2&OLS&RRBLUP1&RRBLUP2\\
\hline
$\mu$&1.83&1.81&2.69&0.33&0.33&0.33\\
$\beta_1$&-0.97&-0.89&-1.54&-2.11&-1.15&-2.11\\
$\beta_2$&1.88&0.71&1.03&0.06&0.09&0.11\\
$h_{1,2}$&-1.14&-0.48&-0.57&-1.14&-0.56&-0.56\\
\hline 
\end{tabular}
\end{table}

We summarize our observations from the reported results as follows:
\begin{itemize}
\item Comparing the two OLS models, the estimated effects $\mu$, $\beta_1$ and $\beta_2$ change, but the estimated interaction $\hat{h}_{1,2}$ as well as $\hat{\mathbf{y}}$ remain unchanged.
\item Comparing the two RRBLUP1 models, both methods give different estimates for all the parameters and the solutions produce different predictions $\hat{\mathbf{y}}$.
\item Comparing the two RRBLUP2 models, both methods give different estimates for $\mu$, $\beta_1$ and $\beta_2$, but the same for $h_{1,2}$, as well as $\hat{\mathbf{y}}$.
\end{itemize}
\end{example}

The different cases presented in Example~\ref{ex:06} have a certain systematic pattern, which we will discuss in the following section. 
%In particular note that the predictions of $\y$ are for $\mbox{\small RR}_{\lambda_h = 1}$ independent of the coding.


\section*{Results} 
The observations made in Example~\ref{ex:06} are explained by following simple proposition
which has several interesting implications.
\begin{proposition}\label{prop:01} Let $\M_{i,\bullet}$ be the $p$ vector of the marker values of individual $i$ and let $f(\M_{i,\bullet}): \mathbb{R}^p \rightarrow \mathbb{R}$ be a polynomial of degree $D$ in the marker data. Moreover, let $\tilde{\M}:= \M - \mathbf{1} \P^t$ be a translation of the marker coding (as in Example~\ref{ex:06}) and let us define a polynomial $\tilde{f}$ in the translated variables $\tilde{\M}$ by $\tilde{f}(\tilde{\M}_{i,\bullet}):= f(\tilde{\M}_{i,\bullet} +  \P^t)=f(\M_{i,\bullet})$. Then for any data $\y$, the sum of squared distances will be identical 
	$$ \sum_{i=1,...,n} (y_i - f(\M_{i,\bullet}))^2 = \sum_{i=1,...,n} (y_i - \tilde{f}(\tilde{\M}_{i,\bullet}))^2 $$
	and for any monomial $m$ of degree $D$, the corresponding coefficient $a_m$ in $f(\M_{i,\bullet})$ and $\tilde{a}_m$ in $\tilde{f}(\tilde{\M}_{i,\bullet})$ will be identical:
	$$a_m = \tilde{a}_m.$$ \qed
\end{proposition}
Proposition~\ref{prop:01} has the very simple statement that if we have a certain fit $f$ based on $\M$, and we use the translated marker coding $\tilde{\M}$ in a second regression, the polynomial $\tilde{f}$ will fit the data with the same quadratic distance but also with the same predictions $\hat{\y}$ (due to the definition of $\tilde{f}$). Moreover, the coefficients of highest degree will be the same.  \\

Since OLS is defined only by the minimal quadratic distance this also means that it is invariant to any translation of the coding, provided that the model structure allows the fit to 
adapt any $f$ to the corresponding $\tilde{f}$ of Proposition~\ref{prop:01}.
To allow this adaption, the possibility to adapt any coefficient of monomials of lower degree is required.
We cannot adapt the regression completely if certain coefficients are forced to zero by the model structure. If a coefficient is equal to zero in $f$, it may be different from zero in $\tilde{f}$.
We will illustrate this with an example. 

\begin{example}[Models without certain terms of intermediate degree]\label{ex:07}
	Let us consider the data $\M$ and $\y$ of Example \ref{ex:06} but with the assumption that marker $2$ does not have an additive effect. Then
	$$\begin{pmatrix}
	\hat{\mu} \\
	\hat{\beta}_1 \\
	\hat{h}_{1,2}
	\end{pmatrix}_{\mbox{\small OLS}} = \begin{pmatrix}
	3.71 \\
	-2.098\\
	-0.012\\
	\end{pmatrix}
	\qquad \mbox{and} \qquad \begin{pmatrix}
	\tilde{\mu} \\
	\tilde{\beta}_1 \\
	\tilde{h}_{1,2}
	\end{pmatrix}_{\mbox{\small OLS}}= \begin{pmatrix}
	0.334\\
	-2.11 \\
	-1.162
	\end{pmatrix} $$
	and also the estimates $\hat{\y}$ and $\tilde{\y}$ are different. 
\end{example}
Example~\ref{ex:07} illustrates that   ``completeness" of the model is required to have the possibility to adapt to translations of the coding. 
More precisely, for any monomial of degree $d$, the model has to include all monomials of lower degree with these variables. If this is not the case, the adapted $\tilde{f}$ may not be a valid fit. Given that the model is ``complete" in this sense, Proposition~\ref{prop:01} has various implications.
The following corollary explains the results observed in our examples and some additional properties of penalized regression methods.  

\begin{corollary}\label{cor:01}
	For all statements it is assumed that penalty factors remain unchanged and that the model is complete in the sense that for any $f$, the corresponding $\tilde{f}$ is a valid fit.
	\begin{itemize}
\item [a)] 	For a model of any degree $D$ , the OLS estimates of the coefficients of highest degree as well as the predictions $\hat{\y}$ are invariant with respect to translations of the marker coding.
		\item [b)] For a regression which only penalizes the coefficients of highest degree $D$, the estimates of the coefficients degree $D$ as well as the predictions $\hat{\y}$ are invariant with respect to translations of the marker coding.
		\item[c)] 	In particular, predictions $\hat{\y}$ of RRBLUP are invariant with respect to translations of the marker coding, since we are dealing with a model of degree 1 and a regression that does not penalize the fixed effect (degree 0).
		\item[d)] An additive least absolute shrinkage and selection operator (LASSO) regression $\ell_1$ penalizing the marker effects but not the intercept is invariant to translations of the markers coding. 

	\end{itemize}
\end{corollary}
Corollary~\ref{cor:01} a) is a result of the OLS method being defined only by the sum of squares and explains why the OLS estimates $\hat{h}_{1,2}$ and $\tilde{h}_{1,2}$ of Example~\ref{ex:06} are identical. Part b) is a results of the following observation: For each $f$, its corresponding $\tilde{f}$ will have the same sum of squared distances and the same coefficients of highest degree (with the translated marker coding). Thus, it will have the same value for the target function of Eq.~(\ref{eq:05}) which we aim to minimize.
Since this is true for any polynomial $f$, it is in particular true for the solution minimizing the target function.
Corollary~\ref{cor:01} b) applied to complete models of degree $1$ gives the result of RRBLUP being invariant to translations of the marker coding which has previously for instance been proven using the mixed model equations (which is slightly more complicated and less general than the argumentation here). Part d) illustrates that these observations also transfer to other types of penalized regressions, for instance LASSO. 

Before, we illustrate the impact on a publicly available data set, we give a small example highlighting cases which are not invariant to translations of the marker coding.
We recommend to use the data of Example~\ref{ex:06} to validate the statements.

\begin{example}~\\
	\begin{itemize} \item[a)] Pure ridge regression (with penalty on $\mu$) is not invariant to translations. 
		\item[b)] RRBLUP with the fixed effect forced to zero is not invariant to translations of the marker coding.
				\item[c)] An extended LASSO $\ell_1$ penalizing additive effects and interactions is not in general invariant to translations of the coding.
	\end{itemize}
\end{example} We will now illustrate the practical relevance our observations on a well investigated publicly available wheat data set. \vspace{0.4cm}\\
{\bf Results on a wheat data set}\\

  
\paragraph{Data} We use the well-investigated wheat data set published by Crossa {\it et al.} \cite{Crossa10}. The data set provides the state of $1279$ DArT markers of $599$ genotyped wheat lines and records on their yield in four different environments. The provided coding of the marker data is a $0,1$ coding. For more details on the data see Crossa {\it et al.} \cite{Crossa10} or the \texttt{R} \cite{RCoreTeam2016R:Computing} package \texttt{BGLR} \cite{BGLR}. \\

\paragraph{Calculating the interaction effects} 
The assessment of the practical impact of translations of the marker coding on the effect estimates is difficult. 
Since in practice, the variance components and thus the penalty factors are estimated on the data, the translations of the marker coding may have an additional indirect effect of changing the penalty factors. Moreover, there may be numerical issues changing the interaction effects when a large number of interactions is included.
Thus, it seems difficult to separate these superposed effects. For this reason, we decided to restrict our model to the $100$ most important markers and their interactions. In more detail, for each environment we perform RRBLUP, chose the $100$ markers with highest absolute effect size and built a model with all pairwise interactions between them. Thus, the corresponding eRRBLUP includes the fixed effect $\mu$, $100$ additive effects and $4450$ interactions. We prefer this approach to an approach of randomly selecting $100$ markers, since approaches of restricting interactions to markers with large additive effects can be found in literature (citation). Moreover, we estimated the variance components only for the allele-frequency centered coding and used the penalty factors also for the estimates with other codings. Analogously, the translational invariance of RRBLUP also holds when the penalty factor remains fixed. For the estimation of the variance components, we used the \texttt{regress} package \cite{clifford2014regress}.\\

We compare three different codings: The original provided ${0,1}$ coding, a version translated by $-0.5$, that is a symmetric $-0.5, 0.5$ coding, and a coding in which the mean of each column is subtracted. We will refer to these codings later as the \emph{original} coding, the \emph{symmetric} coding and \emph{allele-frequency centered} coding.


For each of the environments, we compare the correlation of the estimated $4450$ interaction effects for the three different codings. The results are summarized in Table~\ref{tab:01}. 
We see that the estimates are highly correlated, but not identical. In particular, the effects sizes seem to be more similar between the original $0,1$ coding and the $\pm 0.5$ coding than compared to the allele-frequency centered version.

\begin{table}[ht] \caption{Correlation of the estimates of the $4450$ interactions with different marker coding. Colors indicate which data from which environment was used. 
	Black: Environment 1; Red: Environment 2; Green: Environment 3; Blue: Environment 4.}\label{tab:01}
	\centering
	\begin{tabular}{rrr}
		\hline
		&  $\hat{\h}_{symm}$ & $\hat{\h}_{centered}$\\ 
		\hline
			 $\hat{\h}_{original}$ & \begin{tabular}{rr} 0.97 & \textcolor{red}{0.96}\\ \textcolor{green}{0.95} & \textcolor{blue}{0.95}  \end{tabular} &
		\begin{tabular}{rr} 0.82 & \textcolor{red}{0.84}\\ \textcolor{green}{0.80} & \textcolor{blue}{0.83}  \end{tabular} \\ 
	 $\hat{\h}_{symm}$ & \begin{tabular}{rr} -- & \textcolor{red}{--}\\ \textcolor{green}{--} & \textcolor{blue}{--}  \end{tabular} &
	 \begin{tabular}{rr} 0.86 & \textcolor{red}{0.87}\\ \textcolor{green}{0.85} & \textcolor{blue}{0.88}  \end{tabular} \\ 
	
		\hline 
	\end{tabular}
\end{table}
 

%An outstanding theoretical result for the comparison of pure RR and OLS in the $n >p + 1$ case is that for any regression regression problem, a small $\lambda$ exists such that the mean squared error (MSE) of the RR estimator is smaller than the MSE of OLS. \\

%However, as illustrated above, a motivation to use RR in quantitative genetics is the $p+1 \gg n$ problem, which means a pure OLS approach cannot be used. 
%Still in a mixed model setup, we have the freedom to model some effects as fixed and others as random, which means that we penalize some effects and others not. Moreover, we can
%group predictors, for instance additive effects and interactions, and use individual penalty factors $\lambda_i$ for each group. \\




\section*{Discussion}
The illustrated problem of the coding having an impact on the estimates of interactions in penalizes regressions is essential for quantitative genetics, where Hadamard products are often used to model interaction such as epistasis or gene by environment interaction. In particular this shows once more that the size of these effects should be interpreted with caution, since a biological meaningfulness is not obvious, which is also reflected by the doubtful biologically mechanistic meaning of variance components \cite{Huang16}.\\

Table~\ref{tab:01} illustrates that the estimated effect sizes are different when the coding is altered, but also that the correlation of the estimates is relatively high.
In particular, the correlation of effect sizes for the original coding and the symmetric coding are at least on the level of $0.95$ for the wheat data set and for all environments. 
The correlation of the estimates of each of these codings and the allele-frequency centered coding drops below $0.90$ with a minimum of $0.80$. This also fits to previous observations where the predictive ability of different codings was compared and where the allele-frequency centered coding was found to be different from the codings in which each marker was coded identically \cite{Martini17}. 
There, it was also found that the symmetric coding seems to outperform other codings slightly (with respect to predictive ability). \\

It should be highlighted, that the problem does not seem to be a consequence of non-orthogonality of the predictor variables (marker values and their products), since these problems would not appear in an OLS regression (provided that it exists), where the variables have the same coding. \\

Finally, note that it was reported that Gaussian Kernel regression \cite{Morota14} can be interpreted as a limit of the polynomial regression with increasing degree (and all possible monomials) \cite{jiang15}. 
This raises the question of why the Gaussian Kernel regression is not affected by translations of the marker coding. It may be interesting from a theoretical point of view to reconsider the limit behavior.


\section*{Conclusion}
We identified the cause of the coding-dependent performance of epistasis effects models. Our results were motivated by ridge regression, but do equally hold for many other types of penalized regressions, for instance for the $\ell_1$ penalized LASSO. The fact that the estimated effect sizes depend on the coding in particular underlines again that estimated effect sizes should be treated with caution. Moreover, this problematic of coding is not only present for marker by marker interaction, but for any mixed model in which interactions are modeled by Hadamard products of covariance matrices, in particular also for gene by environment (G x E) models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\begin{center} {\huge Appendix } \end{center}
\vspace{0.5cm}
\begin{proof}[Proposition 1]
	The fact that the goodness of fit remains the same results from the definition of the polynomials. To see that the coefficients of monomials of highest degree are identical, choose a monomial $m(M_{l_1},M_{l_2},...,M_{l_D})$ of the loci $l_1,...,l_D$ of degree $D$ of $f$. Multiplying the factors of $m(\tilde{M}_{l_1}+P_{l_1},\tilde{M}_{l_2}+P_{l_2},...,\tilde{M}_{l_D}+P_{l_D})$ gives the same monomial 
	$m(\tilde{M}_{l_1},\tilde{M}_{l_2},...,\tilde{M}_{l_D})$ as a summand of highest degree, plus additional monomials of lower degree. Thus, the coefficients of monomials of degree $D$ remain the same. 
\end{proof}
\section*{Author's contribution}
JWRM: Proposed to consider the topic; derived the theoretical results; wrote the manuscript
FR, NTH,JH: Verified the results
All authors: Discussed the research



\section*{Acknowledgements}
JWRM thanks KWS SAAT SE for financial support.

%\begin{thebibliography}{99}
%\bibitem{abeliovich16}	   
%	H. Abeliovich, {\it J. Math. Biol.} {\bf 73} (2016) 1--13.  	
%\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
%\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{bmc_article}  % Bibliography file (usually '*.bib' )
% for author-year bibliography (bmc-mathphys or spbasic)
% a) write to bib file (bmc-mathphys only)
% @settings{label, options="nameyear"}
% b) uncomment next line
%\nocite{label}

% or include bibliography directly:


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files


% \textbf{Figure 3:} Heat plots for number of iterations in the reml optimization procedure, for (a) SC1 and (b) SC2. The lighter the colour, the greater the number of iterations.

%\begin{figure}[h!]
  %\caption{\csentence{Sample figure title.}
    %  Figure legend text.}
     % \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
%\section*{Tables}
%\begin{table}[h!]
%\caption{Sample table title. This is where the description of the table should go.}
%      \begin{tabular}{cccc}
%        \hline
%           & B1  &B2   & B3\\ \hline
%        A1 & 0.1 & 0.2 & 0.3\\
%        A2 & ... & ..  & .\\
%        A3 & ..  & .   & .\\ \hline
%      \end{tabular}
%\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section*{Additional Files}
%  \subsection*{Additional file 1 --- Sample additional file title}
%    Additional file descriptions text (including details of how to
%    view the file, if it is in a non-standard format or the file extension).  This might
%    refer to a multi-page table or a figure.
%
%  \subsection*{Additional file 2 --- Sample additional file title}
%    Additional file descriptions text.

\end{document}
